{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "62ccf424-0527-4832-b2c5-318d334e8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "LM_dict=pd.read_excel('LoughranMcDonald_MasterDictionary_2020.xlsx')\n",
    "LM_dict.drop(columns= ['Seq_num', 'Word Count', 'Word Proportion','Average Proportion', 'Std Dev', 'Doc Count','Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal','Constraining', 'Complexity', 'Syllables', 'Source'],inplace=True)\n",
    "\n",
    "\n",
    "LM_dict['Word']=LM_dict['Word'].str.lower()                           #making separate copy for positive and negative words\n",
    "LM_pos=LM_dict.copy()\n",
    "LM_neg=LM_dict.copy()\n",
    "\n",
    "   \n",
    "et=LM_pos[LM_pos['Positive']==0].index                                       #sorting positive values\n",
    "LM_pos.drop(et,inplace=True)\n",
    "LM_pos.drop(['Negative'],axis=1,inplace=True)\n",
    "LM_pos=LM_pos.reset_index(drop=True)\n",
    "\n",
    "\n",
    "rt=LM_neg[LM_neg['Negative']==0].index                                        #sorting negative values\n",
    "LM_neg.drop(rt,inplace=True)\n",
    "LM_neg.drop(['Positive'],axis=1,inplace=True)\n",
    "LM_neg=LM_neg.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    \n",
    "a = open('StopWords_Generic.txt', 'r')                                           #loading stopwords_generic.txt and removing garbage values\n",
    "for i in a:\n",
    "    stpds+=[i[:-1].lower()]    \n",
    "    \n",
    "    \n",
    "unc_dict=pd.read_excel('uncertainty_dictionary.xlsx')                            #loading uncertainty_dictionary.xlsx and converting them to lower\n",
    "unc_dict['Word']=unc_dict['Word'].str.lower()\n",
    "\n",
    "con_dict=pd.read_excel('constraining_dictionary.xlsx')                           #loading constraining_dictionary.xlsx and converting them to lower\n",
    "con_dict['Word']=con_dict['Word'].str.lower()\n",
    "\n",
    "\n",
    "\n",
    "def pos_neg(t):                                                                  #creating a function to pre process the converted text file so that finding positive and negative can done easily that are not present in the provided set of stopwords\n",
    "    all_words=[]\n",
    "    for sent in nltk.sent_tokenize(t.lower()):\n",
    "        words = nltk.word_tokenize(sent)        \n",
    "        words = [word for word in words if word not in stpds] \n",
    "        words = [word for word in words if word not in punctuation]\n",
    "        words = [word for word in words if not word.isnumeric()]\n",
    "        all_words+=words\n",
    "    all_words = ''.join([ch for ch in ' '.join(all_words) if (ch not in punctuation) and (not ch.isnumeric())])\n",
    "    return all_words \n",
    "\n",
    "\n",
    "def texts(t):                                                                        #creating a function to pre process the converted text file so that sentence and words and easily found that are not present in the provided set of stopwords\n",
    "    all_words=[]\n",
    "    for sent in nltk.sent_tokenize(t.lower()):\n",
    "        words = nltk.word_tokenize(sent)        \n",
    "        words = [word for word in words if word not in stpds] \n",
    "        words = [word for word in words if word not in new_punctuation]\n",
    "        words = [word for word in words if not word.isnumeric()]\n",
    "        all_words+=words\n",
    "    all_words = ''.join([ch for ch in ' '.join(all_words) if (ch not in new_punctuation) and (not ch.isnumeric())])\n",
    "    return all_words\n",
    "\n",
    "\n",
    "def syllables(word):                                                                    #creating a function to count no of syllables in a word\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count+=1\n",
    "    if count == 0:\n",
    "        count +=1\n",
    "    return count\n",
    "\n",
    "\n",
    "def cons_repo(t):                                                                               #creating a function to pre process the text file so that constraining words in the whole report can be found as described.\n",
    "    all_words=[]\n",
    "    for sent in nltk.sent_tokenize(t.lower()):\n",
    "        words = nltk.word_tokenize(sent)        \n",
    "        words = [word for word in words if word not in punctuation]\n",
    "        words = [word for word in words if not word.isnumeric()]\n",
    "        all_words+=words\n",
    "    all_words = ''.join([ch for ch in ' '.join(all_words) if (ch not in punctuation) and (not ch.isnumeric())])\n",
    "    return all_words\n",
    "\n",
    "for i in range(2,200):                                                                          #creating a list yu which contain the undesired patterns of \" . \" and space.\n",
    "    p='.'*i\n",
    "    yu+=[p]\n",
    "yu.reverse()\n",
    "for i in range(1,60):  \n",
    "    p='.'+(' '*i)+'.'\n",
    "    yu+=[p]\n",
    "for i in range(2,300):     \n",
    "    p=' '*i\n",
    "    yu+=[p]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cik=pd.read_excel('cik_list.xlsx')                                                         #loading cik_list.xlsx and getting SECFNAME to get links.\n",
    "for i in cik['SECFNAME']:\n",
    "    links+=['https://www.sec.gov/Archives/' + i]\n",
    "    \n",
    "     \n",
    "header={                                                                                                                                                            #header file to connect to site and get the data from links\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\",\n",
    "\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\"\n",
    "}\n",
    "\n",
    "\n",
    "last=pd.DataFrame(columns=['positive_score','negative_score','polarity_score','average_sentence_length','percentage_of_complex_words','fog_index','complex_word_count','word_count','uncertainty_score','constraining_score','positive_word_proportion','negative_word_proportion','uncertainty_word_proportion','constraining_word_proportion','constraining_words_whole_report'])\n",
    "\n",
    "\n",
    "\n",
    "for i in range (0,len(links)):                                      #for loop to run all the links\n",
    "    \n",
    "    \n",
    "    links=[]                                                         # empty variables that are used to store values in the program\n",
    "    stpds=[]       \n",
    "    yu=[]\n",
    "    sas=[]\n",
    "    wees=0\n",
    "    report=''\n",
    "    negative=0\n",
    "    positive=0\n",
    "    complex_wrd=0\n",
    "    word_count=0\n",
    "    constraining_score=0\n",
    "    uncertainity_score=0\n",
    "    cons_wrd_whl_repo=0\n",
    "    new_punctuation=punctuation[:13]+punctuation[14:]\n",
    "  \n",
    "    \n",
    "    r=requests.get(links[i],headers=header)                                               # requesting the links and creating a soup to store the data\n",
    "    text=r.text\n",
    "    soup=BeautifulSoup(text,'html.parser')\n",
    "\n",
    "    \n",
    "    for wui in soup.find_all('page'):                                                       # from the soup we rectified the desired data and removed any html,css,xml present.\n",
    "        report+=wui.get_text()\n",
    "    if not report:\n",
    "        for wui in soup.find_all('text'):\n",
    "            report+=wui.get_text()\n",
    "        \n",
    "        \n",
    "        \n",
    "    ter=pos_neg(report)                                                                     # we used split function to seperate the data from spaces and created a list named ty which contained all the words that will be processed to find no. of positive and negative words\n",
    "    ty=ter.split()\n",
    "    \n",
    "    for g in ty:                                                                            # finding positive , negative words and polarity\n",
    "        if g in LM_neg['Word'].tolist():\n",
    "            negative+=1\n",
    "        if g in LM_pos['Word'].tolist():\n",
    "            positive+=1\n",
    "    polarity=(positive-negative)/((positive+negative)+0.000001)                          \n",
    "    \n",
    "    \n",
    "    wer=texts(report)                                                                       # using text function created above to sort the data accordingling that is then used to find no. of words and sentences.\n",
    "    for u in yu:                                                                            # removing the patterns of \" . \" and spaces from data.\n",
    "        wer=wer.replace(u,' ')\n",
    "        \n",
    "          \n",
    "    sentences=wer.count(' . ')                                                               #counting no. of sentences\n",
    "    if not sentences:\n",
    "        sentences=wer.count('. ')\n",
    "    if not sentences:\n",
    "        sentences=wer.count('.')\n",
    "           \n",
    "        \n",
    "    qw=wer.split()                                                                                                     \n",
    "    for k in qw:                                                                                   \n",
    "        if (len(k)>1) and (k[-1]!='.'):\n",
    "            sas+=[k.lower()]                                                                           # creating a list sas that is later used as it contain all the rectified words\n",
    "            wees+=1                                                                                    # finding no. of words\n",
    "    \n",
    "    avg_sentence_length=wees/sentences                                                       # calculating average sentence length\n",
    "    \n",
    "    for n in sas:                                                                            # finding complex words\n",
    "        if syllables(n)>2:\n",
    "            complex_wrd+=1\n",
    "        \n",
    "    percentage_clx_wrds=complex_wrd/wees                                                    # calculating percentage of complex words\n",
    "    fog_index=0.4*(avg_sentence_length+percentage_clx_wrds)                                 # calculating fog index\n",
    "    \n",
    "    for m in sas:\n",
    "        if m not in stopwords.words('english'):                                              # calculating word count\n",
    "            word_count+=1\n",
    "    \n",
    "    for l in sas:                                                                            # finding uncertainity score\n",
    "        if l in unc_dict['Word'].tolist():\n",
    "            uncertainity_score+=1\n",
    "\n",
    "    for n in sas:                                                                            # finding constraining score\n",
    "        if n in con_dict['Word'].tolist():\n",
    "            constraining_score+=1\n",
    "            \n",
    "    pos_word_proportion=positive/word_count                                                  # finding positive word proportion\n",
    "    \n",
    "    neg_word_proportion=negative/word_count                                                  # finding negative word proportion\n",
    "    \n",
    "    uncertainity_wrd_pro=uncertainity_score/word_count                                       # finding uncertainity word proportion\n",
    "    \n",
    "    constraining_wrd_pro=constraining_score/word_count                                       # finding constraining word proportion\n",
    "    \n",
    "    ret=cons_repo(report)                                                                    # finding constraining word in the whole report \n",
    "    ret=ret.split()\n",
    "    for e in ret:\n",
    "        if e in con_dict['Word'].tolist():\n",
    "            cons_wrd_whl_repo+=1\n",
    "    \n",
    "    \n",
    "    last=last.append({'positive_score':positive,'negative_score':negative,'polarity_score':polarity,'average_sentence_length':avg_sentence_length,'percentage_of_complex_words':percentage_clx_wrds,'fog_index':fog_index,'complex_word_count':complex_wrd,'word_count':word_count,'uncertainty_score':uncertainity_score,'constraining_score':constraining_score,'positive_word_proportion':pos_word_proportion,'negative_word_proportion':neg_word_proportion,'uncertainty_word_proportion':uncertainity_wrd_pro,'constraining_word_proportion':constraining_wrd_pro,'constraining_words_whole_report':cons_wrd_whl_repo },ignore_index=True)\n",
    "\n",
    "                                                                                            # adding calculated values to the dataframa.\n",
    "    \n",
    "resultant=pd.concat([cik,last],axis=1)                                                      # concating cik_list.xlsx with the calculated dataframe sidewise as described.\n",
    "resultant.to_excel(\"resultant.xlsx\")                                                        # saving the created dataframe to the drive.\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
